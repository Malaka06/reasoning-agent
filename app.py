import streamlit as st
import requests

# =========================
# Configuration de la page
# =========================
st.set_page_config(
    page_title="Reasoning Agent ‚Äì Data Analyst / Data Scientist",
    layout="wide"
)

st.title("üß† Reasoning Agent")
st.caption("Simulation de ma fa√ßon de raisonner (Data Analyst / Data Scientist)")

st.info(
    "üëã **Recruteurs** : cette application ne vise pas √† donner des r√©ponses parfaites, "
    "mais √† **montrer comment je raisonne**, structure mes d√©cisions et arbitre entre plusieurs options "
    "face √† des probl√©matiques data r√©elles."
)

st.markdown("""
Pose une question **technique ou m√©tier**.  
L‚Äôagent g√©n√®re :
- une **r√©ponse synth√©tique**
- le **raisonnement √©tape par √©tape**
- des **alternatives volontairement √©cart√©es**
""")

# =========================
# Questions d'exemple
# =========================
examples = [
    "Que fais-tu si les donn√©es sont de mauvaise qualit√© ?",
    "Comment choisis-tu un mod√®le pour un probl√®me de churn ?",
    "Comment traduis-tu un besoin m√©tier flou en analyse data ?",
    "Que fais-tu quand les r√©sultats ne confirment pas l‚Äôhypoth√®se m√©tier ?"
]

selected = st.radio(
    "üí° Exemples de questions (ou √©cris la tienne) :",
    examples
)

question = st.text_area(
    "Ta question",
    value=selected,
    height=120
)

# =========================
# Hugging Face Router
# =========================
MODEL_ID = "moonshotai/Kimi-K2-Instruct-0905"
HF_URL = "https://router.huggingface.co/v1/chat/completions"

HF_HEADERS = {
    "Authorization": f"Bearer {st.secrets['HF_API_TOKEN']}",
    "Content-Type": "application/json",
}

def call_llm(user_question: str) -> str:
    system_prompt = (
        "Tu es un agent qui simule ma fa√ßon de raisonner comme Data Analyst / Data Scientist.\n\n"
        "Tu dois r√©pondre en fran√ßais avec EXACTEMENT la structure suivante :\n\n"
        "R√©ponse :\n"
        "(1‚Äì2 phrases, orient√©es d√©cision)\n\n"
        "Raisonnement :\n"
        "- √©tape 1 : clarification / diagnostic\n"
        "- √©tape 2 : analyse et arbitrages\n"
        "- √©tape 3 : d√©cision finale\n\n"
        "Alternatives :\n"
        "- option 1 + pourquoi je ne la choisis pas\n"
        "- option 2 + pourquoi je ne la choisis pas\n\n"
        "Sois clair, pragmatique et orient√© impact m√©tier."
    )

    payload = {
        "model": MODEL_ID,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_question}
        ],
        "temperature": 0.4,
    }

    r = requests.post(
        HF_URL,
        headers=HF_HEADERS,
        json=payload,
        timeout=90
    )

    if r.status_code != 200:
        raise RuntimeError(f"{r.status_code} - {r.text}")

    data = r.json()
    return data["choices"][0]["message"]["content"]

# =========================
# G√©n√©ration
# =========================
if st.button("üöÄ G√©n√©rer"):
    if not question.strip():
        st.warning("Merci d‚Äô√©crire une question.")
        st.stop()

    with st.spinner("Analyse et raisonnement en cours..."):
        try:
            answer = call_llm(question)
        except Exception as e:
            st.error(f"Erreur IA : {e}")
            st.stop()

    st.markdown(answer)

# =========================
# Explication du projet
# =========================
with st.expander("üîç Comment cet agent a √©t√© con√ßu"):
    st.markdown("""
    **Objectif**
    - Montrer mon *processus de r√©flexion*, pas seulement des r√©sultats.
    - Rendre visible la prise de d√©cision data en contexte m√©tier.

    **Stack**
    - Python
    - Streamlit
    - Hugging Face Router (LLM open-source)

    **Approche**
    - Prompt structur√© pour forcer la clart√© du raisonnement
    - Mise en avant des alternatives non retenues
    - R√©ponses volontairement concises et orient√©es impact

    **Limite assum√©e**
    - Ce n‚Äôest pas un chatbot g√©n√©rique.
    - C‚Äôest une **simulation de ma mani√®re de raisonner**.
    """)
