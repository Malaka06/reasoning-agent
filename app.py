import re
import streamlit as st
import requests

# =========================
# Configuration de la page
# =========================
st.set_page_config(
    page_title="Reasoning Agent â€“ Data Analyst / Data Scientist",
    layout="wide",
    page_icon="ğŸ§ ",
)

st.title("ğŸ§  Reasoning Agent")
st.caption("Simulation de ma faÃ§on de raisonner (Data Analyst / Data Scientist)")

st.info(
    "ğŸ‘‹ **Recruteurs** : cette application ne vise pas Ã  donner des rÃ©ponses parfaites, "
    "mais Ã  **montrer comment je raisonne**, structure mes dÃ©cisions et arbitre entre plusieurs options "
    "face Ã  des problÃ©matiques data rÃ©elles."
)

st.markdown("""
Pose une question **technique ou mÃ©tier**.  
Lâ€™agent gÃ©nÃ¨re :
- une **rÃ©ponse synthÃ©tique**
- le **raisonnement Ã©tape par Ã©tape**
- des **alternatives volontairement Ã©cartÃ©es**
""")

# =========================
# Inputs
# =========================
examples = [
    "Que fais-tu si les donnÃ©es sont de mauvaise qualitÃ© ?",
    "Comment choisis-tu un modÃ¨le pour un problÃ¨me de churn ?",
    "Comment traduis-tu un besoin mÃ©tier flou en analyse data ?",
    "Que fais-tu quand les rÃ©sultats ne confirment pas lâ€™hypothÃ¨se mÃ©tier ?"
]

selected = st.radio("ğŸ’¡ Exemples de questions (ou Ã©cris la tienne) :", examples)

question = st.text_area("Ta question", value=selected, height=120)

default_model = "moonshotai/Kimi-K2-Instruct-0905"
model_id = st.text_input("ModÃ¨le (tu peux changer plus tard)", value=default_model)

with st.expander("âš™ï¸ ParamÃ¨tres (optionnel)"):
    temperature = st.slider("temperature", 0.0, 1.0, 0.4, 0.05)
    max_tokens = st.slider("max_tokens", 128, 1200, 600, 32)

# =========================
# Hugging Face Router
# =========================
HF_URL = "https://router.huggingface.co/v1/chat/completions"
HF_TOKEN = st.secrets.get("HF_API_TOKEN")

if not HF_TOKEN:
    st.error("âŒ Secret manquant : ajoute `HF_API_TOKEN` dans Streamlit â†’ Settings â†’ Secrets.")
    st.stop()

HF_HEADERS = {
    "Authorization": f"Bearer {HF_TOKEN}",
    "Content-Type": "application/json",
}

SYSTEM_PROMPT = (
    "Tu es un agent qui simule ma faÃ§on de raisonner comme Data Analyst / Data Scientist.\n\n"
    "Tu dois rÃ©pondre en franÃ§ais avec EXACTEMENT la structure suivante :\n\n"
    "RÃ©ponse :\n"
    "(1â€“2 phrases, orientÃ©es dÃ©cision)\n\n"
    "Raisonnement :\n"
    "- Ã©tape 1 : clarification / diagnostic\n"
    "- Ã©tape 2 : analyse et arbitrages\n"
    "- Ã©tape 3 : dÃ©cision finale\n\n"
    "Preuves :\n"
    "- 2â€“3 critÃ¨res / signaux concrets que tu utiliserais (mÃªme sans donnÃ©es sous les yeux)\n\n"
    "Alternatives :\n"
    "- option 1 + pourquoi je ne la choisis pas\n"
    "- option 2 + pourquoi je ne la choisis pas\n\n"
    "Sois clair, pragmatique et orientÃ© impact mÃ©tier."
)

@st.cache_data(show_spinner=False, ttl=3600)
def call_llm_cached(model: str, q: str, temp: float, max_toks: int) -> str:
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": q}
        ],
        "temperature": temp,
        "max_tokens": max_toks,
    }

    r = requests.post(HF_URL, headers=HF_HEADERS, json=payload, timeout=90)

    if r.status_code != 200:
        # On affiche un extrait pour Ã©viter un pavÃ© illisible
        raise RuntimeError(f"{r.status_code} - {r.text[:800]}")

    data = r.json()
    return data["choices"][0]["message"]["content"]

def split_sections(text: str) -> dict:
    """
    Essaie de dÃ©couper RÃ©ponse / Raisonnement / Preuves / Alternatives.
    Si le modÃ¨le ne respecte pas parfaitement, on fait au mieux.
    """
    sections = {"RÃ©ponse": "", "Raisonnement": "", "Preuves": "", "Alternatives": ""}

    # Normalise
    t = text.strip()

    # Regex basique sur titres
    pattern = r"(RÃ©ponse\s*:|Raisonnement\s*:|Preuves\s*:|Alternatives\s*:)"
    parts = re.split(pattern, t)

    if len(parts) <= 1:
        sections["RÃ©ponse"] = t
        return sections

    current = None
    for chunk in parts:
        c = chunk.strip()
        if not c:
            continue
        if c.startswith("RÃ©ponse"):
            current = "RÃ©ponse"
            continue
        if c.startswith("Raisonnement"):
            current = "Raisonnement"
            continue
        if c.startswith("Preuves"):
            current = "Preuves"
            continue
        if c.startswith("Alternatives"):
            current = "Alternatives"
            continue
        if current:
            sections[current] += (c + "\n")

    # Clean
    for k in sections:
        sections[k] = sections[k].strip()

    return sections

# =========================
# GÃ©nÃ©ration
# =========================
if st.button("ğŸš€ GÃ©nÃ©rer"):
    if not question.strip():
        st.warning("Merci dâ€™Ã©crire une question.")
        st.stop()

    with st.spinner("Analyse et raisonnement en cours..."):
        try:
            raw = call_llm_cached(model_id, question, temperature, max_tokens)
        except Exception as e:
            st.error(f"Erreur IA : {e}")
            st.stop()

    sec = split_sections(raw)

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("âœ… RÃ©ponse")
        st.write(sec["RÃ©ponse"] or "â€”")

        st.subheader("ğŸ§¾ Preuves (critÃ¨res)")
        st.write(sec["Preuves"] or "â€”")

    with col2:
        st.subheader("ğŸ§  Raisonnement")
        st.write(sec["Raisonnement"] or "â€”")

        st.subheader("ğŸ” Alternatives")
        st.write(sec["Alternatives"] or "â€”")

# =========================
# Explication du projet
# =========================
with st.expander("ğŸ” Comment cet agent a Ã©tÃ© conÃ§u"):
    st.markdown("""
**Objectif**
- Montrer mon *processus de rÃ©flexion*, pas seulement des rÃ©sultats.
- Rendre visible la prise de dÃ©cision data en contexte mÃ©tier.

**Stack**
- Python
- Streamlit
- Hugging Face Router (LLM)

**Approche**
- Prompt structurÃ© pour rendre explicite le raisonnement
- Mise en avant des alternatives non retenues
- RÃ©ponses concises et orientÃ©es impact

**Limite assumÃ©e**
- Ce nâ€™est pas un chatbot gÃ©nÃ©rique.
- Câ€™est une **simulation de ma maniÃ¨re de raisonner**.
""")
